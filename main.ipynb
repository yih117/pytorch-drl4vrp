{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!rm -rf pytorch-drl4vrp\n",
        "!rm -rf vrp"
      ],
      "metadata": {
        "id": "PEVLu-2xVJFi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f2iqcoJQLAi",
        "outputId": "93408b99-c4e6-4ae7-b8aa-721c5eed8ce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-drl4vrp'...\n",
            "warning: redirecting to https://github.com/yih117/pytorch-drl4vrp.git/\n",
            "remote: Enumerating objects: 574, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 574 (delta 26), reused 0 (delta 0), pack-reused 524\u001b[K\n",
            "Receiving objects: 100% (574/574), 1.41 MiB | 36.21 MiB/s, done.\n",
            "Resolving deltas: 100% (285/285), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone http://github.com/yih117/pytorch-drl4vrp.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python pytorch-drl4vrp/trainer.py --task=vrp --nodes=10 --batch_size=128 --train-size=1000000 --max_time=3 --actor_lr=1e-4 --critic_lr=1e-4 --hidden=256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWqvm4e2Rl5J",
        "outputId": "f957d29c-51c1-4c22-e15e-7053aafd934c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Actor parameters: 725760\n",
            "Number of Actor parameters: 12749\n",
            "/content/pytorch-drl4vrp/tasks/vrp.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(tensor.data, device=dynamic.device), cumulative_reward\n",
            "  Batch 99/7813, reward: -1.232, loss: 0.3286, took: 19.9286s\n",
            "  Batch 199/7813, reward: -1.230, loss: 0.3234, took: 19.0205s\n",
            "  Batch 299/7813, reward: -1.234, loss: 0.4161, took: 18.6057s\n",
            "  Batch 399/7813, reward: -1.234, loss: 0.3376, took: 19.2680s\n",
            "  Batch 499/7813, reward: -1.231, loss: 0.3818, took: 18.2166s\n",
            "  Batch 599/7813, reward: -1.231, loss: 0.5013, took: 19.8532s\n",
            "  Batch 699/7813, reward: -1.236, loss: 0.3612, took: 18.2920s\n",
            "  Batch 799/7813, reward: -1.226, loss: 0.3854, took: 19.3633s\n",
            "  Batch 899/7813, reward: -1.235, loss: 0.4406, took: 18.5154s\n",
            "  Batch 999/7813, reward: -1.236, loss: 0.5481, took: 18.9970s\n",
            "  Batch 1099/7813, reward: -1.233, loss: 0.5390, took: 18.2870s\n",
            "  Batch 1199/7813, reward: -1.234, loss: 0.3789, took: 18.4605s\n",
            "  Batch 1299/7813, reward: -1.237, loss: 0.3332, took: 18.8479s\n",
            "  Batch 1399/7813, reward: -1.236, loss: 0.3037, took: 18.4807s\n",
            "  Batch 1499/7813, reward: -1.239, loss: 0.4145, took: 18.7713s\n",
            "  Batch 1599/7813, reward: -1.238, loss: 0.4266, took: 18.4935s\n",
            "  Batch 1699/7813, reward: -1.243, loss: 0.3975, took: 18.7759s\n",
            "  Batch 1799/7813, reward: -1.251, loss: 0.4200, took: 18.5553s\n",
            "  Batch 1899/7813, reward: -1.250, loss: 0.3838, took: 19.1213s\n",
            "  Batch 1999/7813, reward: -1.245, loss: 0.4219, took: 18.8676s\n",
            "  Batch 2099/7813, reward: -1.254, loss: 0.4267, took: 19.1377s\n",
            "  Batch 2199/7813, reward: -1.264, loss: 0.4053, took: 18.7619s\n",
            "  Batch 2299/7813, reward: -1.256, loss: 0.3661, took: 19.2107s\n",
            "  Batch 2399/7813, reward: -1.258, loss: 0.4384, took: 18.6168s\n",
            "  Batch 2499/7813, reward: -1.254, loss: 0.4525, took: 19.1838s\n",
            "  Batch 2599/7813, reward: -1.267, loss: 0.3531, took: 18.5344s\n",
            "  Batch 2699/7813, reward: -1.263, loss: 0.3450, took: 19.3345s\n",
            "  Batch 2799/7813, reward: -1.264, loss: 0.4138, took: 18.4808s\n",
            "  Batch 2899/7813, reward: -1.267, loss: 0.3122, took: 19.0247s\n",
            "  Batch 2999/7813, reward: -1.267, loss: 0.4461, took: 19.1929s\n",
            "  Batch 3099/7813, reward: -1.265, loss: 0.3505, took: 19.0947s\n",
            "  Batch 3199/7813, reward: -1.267, loss: 0.3853, took: 18.4835s\n",
            "  Batch 3299/7813, reward: -1.266, loss: 0.3575, took: 18.8725s\n",
            "  Batch 3399/7813, reward: -1.267, loss: 0.3222, took: 18.7940s\n",
            "  Batch 3499/7813, reward: -1.271, loss: 0.4121, took: 18.6609s\n",
            "  Batch 3599/7813, reward: -1.272, loss: 0.3724, took: 19.4156s\n",
            "  Batch 3699/7813, reward: -1.276, loss: 0.3051, took: 18.7643s\n",
            "  Batch 3799/7813, reward: -1.271, loss: 0.4719, took: 19.3898s\n",
            "  Batch 3899/7813, reward: -1.274, loss: 0.2781, took: 18.5452s\n",
            "  Batch 3999/7813, reward: -1.278, loss: 0.3289, took: 19.8960s\n",
            "  Batch 4099/7813, reward: -1.277, loss: 0.4502, took: 18.7109s\n",
            "  Batch 4199/7813, reward: -1.280, loss: 0.3016, took: 19.3126s\n",
            "  Batch 4299/7813, reward: -1.283, loss: 0.3517, took: 18.6365s\n",
            "  Batch 4399/7813, reward: -1.282, loss: 0.2953, took: 19.6536s\n",
            "  Batch 4499/7813, reward: -1.280, loss: 0.3567, took: 18.2316s\n",
            "  Batch 4599/7813, reward: -1.285, loss: 0.2959, took: 19.5507s\n",
            "  Batch 4699/7813, reward: -1.282, loss: 0.3593, took: 19.2928s\n",
            "  Batch 4799/7813, reward: -1.292, loss: 0.3431, took: 18.4322s\n",
            "  Batch 4899/7813, reward: -1.284, loss: 0.3425, took: 19.0827s\n",
            "  Batch 4999/7813, reward: -1.285, loss: 0.4011, took: 19.5432s\n",
            "  Batch 5099/7813, reward: -1.287, loss: 0.3664, took: 19.4266s\n",
            "  Batch 5199/7813, reward: -1.291, loss: 0.3674, took: 18.9128s\n",
            "  Batch 5299/7813, reward: -1.289, loss: 0.3031, took: 19.3570s\n",
            "  Batch 5399/7813, reward: -1.289, loss: 0.3237, took: 19.4508s\n",
            "  Batch 5499/7813, reward: -1.297, loss: 0.2925, took: 19.4299s\n",
            "  Batch 5599/7813, reward: -1.297, loss: 0.3249, took: 19.2295s\n",
            "  Batch 5699/7813, reward: -1.296, loss: 0.3418, took: 18.2686s\n",
            "  Batch 5799/7813, reward: -1.301, loss: 0.2238, took: 19.1404s\n",
            "  Batch 5899/7813, reward: -1.304, loss: 0.3365, took: 18.1932s\n",
            "  Batch 5999/7813, reward: -1.304, loss: 0.3165, took: 19.0078s\n",
            "  Batch 6099/7813, reward: -1.313, loss: 0.3948, took: 18.8716s\n",
            "  Batch 6199/7813, reward: -1.311, loss: 0.2660, took: 18.5200s\n",
            "  Batch 6299/7813, reward: -1.317, loss: 0.2977, took: 18.7384s\n",
            "  Batch 6399/7813, reward: -1.316, loss: 0.3550, took: 19.1408s\n",
            "  Batch 6499/7813, reward: -1.323, loss: 0.2448, took: 18.3811s\n",
            "  Batch 6599/7813, reward: -1.324, loss: 0.2922, took: 19.2649s\n",
            "  Batch 6699/7813, reward: -1.335, loss: 0.3250, took: 18.5876s\n",
            "  Batch 6799/7813, reward: -1.336, loss: 0.2960, took: 19.2419s\n",
            "  Batch 6899/7813, reward: -1.344, loss: 0.2386, took: 18.3638s\n",
            "  Batch 6999/7813, reward: -1.353, loss: 0.3175, took: 18.9514s\n",
            "  Batch 7099/7813, reward: -1.358, loss: 0.2640, took: 18.4403s\n",
            "  Batch 7199/7813, reward: -1.364, loss: 0.2430, took: 19.5146s\n",
            "  Batch 7299/7813, reward: -1.365, loss: 0.2505, took: 18.4251s\n",
            "  Batch 7399/7813, reward: -1.372, loss: 0.2791, took: 19.0453s\n",
            "  Batch 7499/7813, reward: -1.376, loss: 0.2030, took: 19.4174s\n",
            "  Batch 7599/7813, reward: -1.380, loss: 0.2467, took: 18.9525s\n",
            "  Batch 7699/7813, reward: -1.398, loss: 0.2148, took: 18.9834s\n",
            "  Batch 7799/7813, reward: -1.398, loss: 0.1948, took: 18.3438s\n",
            "Mean epoch loss/reward: 0.3482, -1.2838, -1.5385, 1.5919 took: 1498.1627s (18.9252s / 100 batches)\n",
            "\n",
            "  Batch 99/7813, reward: -1.404, loss: 0.1792, took: 18.3496s\n",
            "  Batch 199/7813, reward: -1.408, loss: 0.1725, took: 19.1234s\n",
            "  Batch 299/7813, reward: -1.410, loss: 0.1524, took: 18.2210s\n",
            "  Batch 399/7813, reward: -1.421, loss: 0.2033, took: 19.9239s\n",
            "  Batch 499/7813, reward: -1.426, loss: 0.1416, took: 19.1812s\n",
            "  Batch 599/7813, reward: -1.432, loss: 0.0876, took: 19.6342s\n",
            "  Batch 699/7813, reward: -1.430, loss: 0.1409, took: 18.8468s\n",
            "  Batch 799/7813, reward: -1.437, loss: 0.0986, took: 18.5891s\n",
            "  Batch 899/7813, reward: -1.441, loss: 0.1244, took: 19.2604s\n",
            "  Batch 999/7813, reward: -1.444, loss: 0.0980, took: 19.0337s\n",
            "  Batch 1099/7813, reward: -1.449, loss: 0.1061, took: 19.4126s\n",
            "  Batch 1199/7813, reward: -1.456, loss: 0.0931, took: 18.5619s\n",
            "  Batch 1299/7813, reward: -1.456, loss: 0.0707, took: 19.4004s\n",
            "  Batch 1399/7813, reward: -1.459, loss: 0.0641, took: 19.2680s\n",
            "  Batch 1499/7813, reward: -1.466, loss: 0.1033, took: 19.2072s\n",
            "  Batch 1599/7813, reward: -1.468, loss: 0.1214, took: 19.0273s\n",
            "  Batch 1699/7813, reward: -1.468, loss: 0.1028, took: 18.6421s\n",
            "  Batch 1799/7813, reward: -1.472, loss: 0.0591, took: 18.9251s\n",
            "  Batch 1899/7813, reward: -1.470, loss: 0.1059, took: 18.5335s\n",
            "  Batch 1999/7813, reward: -1.472, loss: 0.1028, took: 18.9398s\n",
            "  Batch 2099/7813, reward: -1.474, loss: 0.0624, took: 18.7464s\n",
            "  Batch 2199/7813, reward: -1.477, loss: 0.0813, took: 19.2830s\n",
            "  Batch 2299/7813, reward: -1.482, loss: 0.0822, took: 18.6118s\n",
            "  Batch 2399/7813, reward: -1.480, loss: 0.0917, took: 18.9498s\n",
            "  Batch 2499/7813, reward: -1.492, loss: 0.0830, took: 19.4629s\n",
            "  Batch 2599/7813, reward: -1.489, loss: 0.0716, took: 18.9979s\n",
            "  Batch 2699/7813, reward: -1.491, loss: 0.0638, took: 18.5434s\n",
            "  Batch 2799/7813, reward: -1.494, loss: 0.0857, took: 19.3269s\n",
            "  Batch 2899/7813, reward: -1.497, loss: 0.0724, took: 18.2436s\n",
            "  Batch 2999/7813, reward: -1.493, loss: 0.0903, took: 18.8381s\n",
            "  Batch 3099/7813, reward: -1.494, loss: 0.0420, took: 18.7675s\n",
            "  Batch 3199/7813, reward: -1.501, loss: 0.0613, took: 18.5512s\n",
            "  Batch 3299/7813, reward: -1.501, loss: 0.0353, took: 18.7024s\n",
            "  Batch 3399/7813, reward: -1.500, loss: 0.0790, took: 18.3821s\n",
            "  Batch 3499/7813, reward: -1.499, loss: 0.0627, took: 18.3319s\n",
            "  Batch 3599/7813, reward: -1.504, loss: 0.0543, took: 19.1688s\n",
            "  Batch 3699/7813, reward: -1.507, loss: 0.0395, took: 18.7764s\n",
            "  Batch 3799/7813, reward: -1.507, loss: 0.0450, took: 17.8928s\n",
            "  Batch 3899/7813, reward: -1.511, loss: 0.0709, took: 17.7936s\n",
            "  Batch 3999/7813, reward: -1.510, loss: 0.0596, took: 18.2905s\n",
            "  Batch 4099/7813, reward: -1.509, loss: 0.0569, took: 17.8597s\n",
            "  Batch 4199/7813, reward: -1.512, loss: 0.0281, took: 18.2214s\n",
            "  Batch 4299/7813, reward: -1.514, loss: 0.0476, took: 17.5036s\n",
            "  Batch 4399/7813, reward: -1.515, loss: 0.0718, took: 18.8647s\n",
            "  Batch 4499/7813, reward: -1.518, loss: -0.0036, took: 18.1881s\n",
            "  Batch 4599/7813, reward: -1.520, loss: 0.0556, took: 18.5233s\n",
            "  Batch 4699/7813, reward: -1.521, loss: 0.0372, took: 17.9912s\n",
            "  Batch 4799/7813, reward: -1.521, loss: 0.0419, took: 19.3993s\n",
            "  Batch 4899/7813, reward: -1.524, loss: 0.0228, took: 18.2907s\n",
            "  Batch 4999/7813, reward: -1.525, loss: 0.0383, took: 18.6030s\n",
            "  Batch 5099/7813, reward: -1.527, loss: 0.0349, took: 18.2645s\n",
            "  Batch 5199/7813, reward: -1.529, loss: 0.0112, took: 18.5855s\n",
            "  Batch 5299/7813, reward: -1.525, loss: 0.0437, took: 17.6332s\n",
            "  Batch 5399/7813, reward: -1.533, loss: 0.0316, took: 18.6745s\n",
            "  Batch 5499/7813, reward: -1.528, loss: 0.0207, took: 17.9175s\n",
            "  Batch 5599/7813, reward: -1.527, loss: 0.0216, took: 18.2099s\n",
            "  Batch 5699/7813, reward: -1.532, loss: 0.0450, took: 17.9258s\n",
            "  Batch 5799/7813, reward: -1.531, loss: 0.0455, took: 18.7932s\n",
            "  Batch 5899/7813, reward: -1.533, loss: 0.0178, took: 18.0922s\n",
            "  Batch 5999/7813, reward: -1.531, loss: 0.0504, took: 18.8052s\n",
            "  Batch 6099/7813, reward: -1.535, loss: 0.0496, took: 17.9993s\n",
            "  Batch 6199/7813, reward: -1.534, loss: 0.0472, took: 18.3330s\n",
            "  Batch 6299/7813, reward: -1.535, loss: 0.0215, took: 17.6838s\n",
            "  Batch 6399/7813, reward: -1.534, loss: 0.0363, took: 18.8053s\n",
            "  Batch 6499/7813, reward: -1.536, loss: 0.0094, took: 36.8867s\n",
            "  Batch 6599/7813, reward: -1.535, loss: 0.0403, took: 18.2502s\n",
            "  Batch 6699/7813, reward: -1.539, loss: 0.0560, took: 18.8839s\n",
            "  Batch 6799/7813, reward: -1.543, loss: 0.0178, took: 18.2943s\n",
            "  Batch 6899/7813, reward: -1.537, loss: 0.0194, took: 18.8521s\n",
            "  Batch 6999/7813, reward: -1.541, loss: 0.0042, took: 19.1024s\n",
            "  Batch 7099/7813, reward: -1.544, loss: 0.0162, took: 18.5052s\n",
            "  Batch 7199/7813, reward: -1.542, loss: 0.0377, took: 18.5162s\n",
            "  Batch 7299/7813, reward: -1.548, loss: 0.0284, took: 19.0711s\n",
            "  Batch 7399/7813, reward: -1.545, loss: 0.0236, took: 18.4678s\n",
            "  Batch 7499/7813, reward: -1.543, loss: 0.0238, took: 19.2459s\n",
            "  Batch 7599/7813, reward: -1.546, loss: 0.0336, took: 18.3729s\n",
            "  Batch 7699/7813, reward: -1.547, loss: 0.0482, took: 18.4282s\n",
            "  Batch 7799/7813, reward: -1.548, loss: 0.0194, took: 18.5324s\n",
            "Mean epoch loss/reward: 0.0630, -1.5000, -1.5876, 1.5919 took: 1493.7805s (18.8759s / 100 batches)\n",
            "\n",
            "  Batch 99/7813, reward: -1.540, loss: 0.0351, took: 19.3852s\n",
            "  Batch 199/7813, reward: -1.549, loss: 0.0181, took: 18.8379s\n",
            "  Batch 299/7813, reward: -1.550, loss: 0.0187, took: 18.8847s\n",
            "  Batch 399/7813, reward: -1.551, loss: 0.0250, took: 18.1059s\n",
            "  Batch 499/7813, reward: -1.554, loss: 0.0328, took: 18.9409s\n",
            "  Batch 599/7813, reward: -1.552, loss: 0.0302, took: 18.8312s\n",
            "  Batch 699/7813, reward: -1.552, loss: 0.0370, took: 18.8385s\n",
            "  Batch 799/7813, reward: -1.550, loss: 0.0246, took: 18.3187s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C_u8Zy3Svuav"
      }
    }
  ]
}