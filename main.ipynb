{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!rm -rf pytorch-drl4vrp"
      ],
      "metadata": {
        "id": "PEVLu-2xVJFi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f2iqcoJQLAi",
        "outputId": "3d8b8045-2fb8-4889-8255-68cc6f64837d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-drl4vrp'...\n",
            "warning: redirecting to https://github.com/yih117/pytorch-drl4vrp.git/\n",
            "remote: Enumerating objects: 559, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 559 (delta 17), reused 0 (delta 0), pack-reused 524\u001b[K\n",
            "Receiving objects: 100% (559/559), 1.41 MiB | 3.33 MiB/s, done.\n",
            "Resolving deltas: 100% (276/276), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone http://github.com/yih117/pytorch-drl4vrp.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python pytorch-drl4vrp/trainer.py --task=vrp --nodes=10 --batch_size=256 --train-size=1000000 --max_time=3 --actor_lr=1e-3 --critic_lr=1e-3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWqvm4e2Rl5J",
        "outputId": "60930d73-be91-4bc4-c1f5-91d04a4a5614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Actor parameters: 182656\n",
            "Number of Actor parameters: 6605\n",
            "/content/pytorch-drl4vrp/tasks/vrp.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(tensor.data, device=dynamic.device), cumulative_reward\n",
            "  Batch 99/3907, reward: -1.232, loss: 0.6422, took: 32.5766s\n",
            "  Batch 199/3907, reward: -1.234, loss: 0.4042, took: 31.5789s\n",
            "  Batch 299/3907, reward: -1.233, loss: 0.7186, took: 31.7962s\n",
            "  Batch 399/3907, reward: -1.235, loss: 0.1464, took: 57.6239s\n",
            "  Batch 499/3907, reward: -1.234, loss: 0.4960, took: 31.1166s\n",
            "  Batch 599/3907, reward: -1.233, loss: 0.5303, took: 56.4586s\n",
            "  Batch 699/3907, reward: -1.235, loss: 0.4165, took: 32.0289s\n",
            "  Batch 799/3907, reward: -1.233, loss: 0.5046, took: 31.3826s\n",
            "  Batch 899/3907, reward: -1.236, loss: 0.3973, took: 32.2307s\n",
            "  Batch 999/3907, reward: -1.235, loss: 0.5504, took: 32.1474s\n",
            "  Batch 1099/3907, reward: -1.236, loss: 0.5771, took: 31.8257s\n",
            "  Batch 1199/3907, reward: -1.229, loss: 0.4441, took: 31.9811s\n",
            "  Batch 1299/3907, reward: -1.238, loss: 0.4369, took: 31.5680s\n",
            "  Batch 1399/3907, reward: -1.236, loss: 0.3950, took: 31.5646s\n",
            "  Batch 1499/3907, reward: -1.234, loss: 0.4357, took: 31.5264s\n",
            "  Batch 1599/3907, reward: -1.234, loss: 0.4944, took: 31.8710s\n",
            "  Batch 1699/3907, reward: -1.240, loss: 0.4527, took: 31.9087s\n",
            "  Batch 1799/3907, reward: -1.238, loss: 0.5099, took: 31.4259s\n",
            "  Batch 1899/3907, reward: -1.235, loss: 0.5176, took: 32.4353s\n",
            "  Batch 1999/3907, reward: -1.236, loss: 0.4557, took: 31.5151s\n",
            "  Batch 2099/3907, reward: -1.232, loss: 0.4470, took: 31.8497s\n",
            "  Batch 2199/3907, reward: -1.236, loss: 0.5027, took: 31.7784s\n",
            "  Batch 2299/3907, reward: -1.235, loss: 0.4358, took: 31.5229s\n",
            "  Batch 2399/3907, reward: -1.258, loss: 0.4096, took: 32.0172s\n",
            "  Batch 2499/3907, reward: -1.346, loss: 0.2436, took: 31.7971s\n",
            "  Batch 2599/3907, reward: -1.390, loss: 0.1629, took: 32.5788s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C_u8Zy3Svuav"
      }
    }
  ]
}