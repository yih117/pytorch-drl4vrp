{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!rm -rf pytorch-drl4vrp"
      ],
      "metadata": {
        "id": "PEVLu-2xVJFi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f2iqcoJQLAi",
        "outputId": "e2409cb7-23ee-4884-f70f-a7493f479d35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-drl4vrp'...\n",
            "warning: redirecting to https://github.com/yih117/pytorch-drl4vrp.git/\n",
            "remote: Enumerating objects: 562, done.\u001b[K\n",
            "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 562 (delta 18), reused 0 (delta 0), pack-reused 524\u001b[K\n",
            "Receiving objects: 100% (562/562), 1.41 MiB | 6.43 MiB/s, done.\n",
            "Resolving deltas: 100% (277/277), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone http://github.com/yih117/pytorch-drl4vrp.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python pytorch-drl4vrp/trainer.py --task=vrp --nodes=10 --batch_size=128 --train-size=1000000 --max_time=3 --actor_lr=1e-4 --critic_lr=1e-4 --hidden=256"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWqvm4e2Rl5J",
        "outputId": "640d6563-5e1d-4f5e-c1d5-195d3a39044a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Actor parameters: 725760\n",
            "Number of Actor parameters: 12749\n",
            "/content/pytorch-drl4vrp/tasks/vrp.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(tensor.data, device=dynamic.device), cumulative_reward\n",
            "  Batch 99/7813, reward: -1.232, loss: 0.3286, took: 18.6212s\n",
            "  Batch 199/7813, reward: -1.230, loss: 0.3234, took: 17.5472s\n",
            "  Batch 299/7813, reward: -1.234, loss: 0.4161, took: 17.0780s\n",
            "  Batch 399/7813, reward: -1.234, loss: 0.3376, took: 17.8280s\n",
            "  Batch 499/7813, reward: -1.231, loss: 0.3818, took: 16.9685s\n",
            "  Batch 599/7813, reward: -1.231, loss: 0.5013, took: 17.1294s\n",
            "  Batch 699/7813, reward: -1.236, loss: 0.3654, took: 17.4748s\n",
            "  Batch 799/7813, reward: -1.229, loss: 0.4796, took: 17.1310s\n",
            "  Batch 899/7813, reward: -1.232, loss: 0.4366, took: 17.5016s\n",
            "  Batch 999/7813, reward: -1.232, loss: 0.4619, took: 17.0956s\n",
            "  Batch 1099/7813, reward: -1.234, loss: 0.4191, took: 17.7946s\n",
            "  Batch 1199/7813, reward: -1.232, loss: 0.4931, took: 17.0626s\n",
            "  Batch 1299/7813, reward: -1.234, loss: 0.4562, took: 17.3348s\n",
            "  Batch 1399/7813, reward: -1.234, loss: 0.3864, took: 18.0497s\n",
            "  Batch 1499/7813, reward: -1.237, loss: 0.4102, took: 16.9489s\n",
            "  Batch 1599/7813, reward: -1.231, loss: 0.4905, took: 17.9183s\n",
            "  Batch 1699/7813, reward: -1.236, loss: 0.3564, took: 17.0627s\n",
            "  Batch 1799/7813, reward: -1.250, loss: 0.4312, took: 17.7323s\n",
            "  Batch 1899/7813, reward: -1.245, loss: 0.4288, took: 17.1581s\n",
            "  Batch 1999/7813, reward: -1.241, loss: 0.4649, took: 17.3217s\n",
            "  Batch 2099/7813, reward: -1.249, loss: 0.4359, took: 17.6438s\n",
            "  Batch 2199/7813, reward: -1.253, loss: 0.4005, took: 17.1409s\n",
            "  Batch 2299/7813, reward: -1.254, loss: 0.4377, took: 17.7607s\n",
            "  Batch 2399/7813, reward: -1.257, loss: 0.3186, took: 16.8455s\n",
            "  Batch 2499/7813, reward: -1.246, loss: 0.3959, took: 17.7588s\n",
            "  Batch 2599/7813, reward: -1.261, loss: 0.4482, took: 17.1291s\n",
            "  Batch 2699/7813, reward: -1.257, loss: 0.4144, took: 17.2558s\n",
            "  Batch 2799/7813, reward: -1.258, loss: 0.3202, took: 17.4935s\n",
            "  Batch 2899/7813, reward: -1.262, loss: 0.3698, took: 16.9726s\n",
            "  Batch 2999/7813, reward: -1.262, loss: 0.3799, took: 17.6941s\n",
            "  Batch 3099/7813, reward: -1.262, loss: 0.3808, took: 16.9290s\n",
            "  Batch 3199/7813, reward: -1.264, loss: 0.3940, took: 17.6855s\n",
            "  Batch 3299/7813, reward: -1.268, loss: 0.3757, took: 16.8787s\n",
            "  Batch 3399/7813, reward: -1.264, loss: 0.3409, took: 16.9524s\n",
            "  Batch 3499/7813, reward: -1.267, loss: 0.2853, took: 17.8147s\n",
            "  Batch 3599/7813, reward: -1.282, loss: 0.3746, took: 17.7169s\n",
            "  Batch 3699/7813, reward: -1.278, loss: 0.3625, took: 18.4500s\n",
            "  Batch 3799/7813, reward: -1.282, loss: 0.3704, took: 17.6015s\n",
            "  Batch 3899/7813, reward: -1.282, loss: 0.3473, took: 17.8753s\n",
            "  Batch 3999/7813, reward: -1.281, loss: 0.3130, took: 17.1314s\n",
            "  Batch 4099/7813, reward: -1.289, loss: 0.3451, took: 17.8444s\n",
            "  Batch 4199/7813, reward: -1.294, loss: 0.4721, took: 17.3607s\n",
            "  Batch 4299/7813, reward: -1.296, loss: 0.4193, took: 17.1388s\n",
            "  Batch 4399/7813, reward: -1.298, loss: 0.2821, took: 18.0439s\n",
            "  Batch 4499/7813, reward: -1.300, loss: 0.4111, took: 17.4942s\n",
            "  Batch 4599/7813, reward: -1.312, loss: 0.3941, took: 18.0348s\n",
            "  Batch 4699/7813, reward: -1.310, loss: 0.3459, took: 17.0182s\n",
            "  Batch 4799/7813, reward: -1.322, loss: 0.2393, took: 17.8068s\n",
            "  Batch 4899/7813, reward: -1.320, loss: 0.3033, took: 17.4450s\n",
            "  Batch 4999/7813, reward: -1.326, loss: 0.2969, took: 17.4760s\n",
            "  Batch 5099/7813, reward: -1.329, loss: 0.3319, took: 17.1927s\n",
            "  Batch 5199/7813, reward: -1.340, loss: 0.2197, took: 17.1660s\n",
            "  Batch 5299/7813, reward: -1.341, loss: 0.2326, took: 18.2862s\n",
            "  Batch 5399/7813, reward: -1.348, loss: 0.2346, took: 17.0019s\n",
            "  Batch 5499/7813, reward: -1.353, loss: 0.2824, took: 17.6975s\n",
            "  Batch 5599/7813, reward: -1.363, loss: 0.2414, took: 17.1615s\n",
            "  Batch 5699/7813, reward: -1.364, loss: 0.1773, took: 17.5722s\n",
            "  Batch 5799/7813, reward: -1.372, loss: 0.1616, took: 17.6468s\n",
            "  Batch 5899/7813, reward: -1.384, loss: 0.2364, took: 17.3351s\n",
            "  Batch 5999/7813, reward: -1.384, loss: 0.2568, took: 17.3465s\n",
            "  Batch 6099/7813, reward: -1.391, loss: 0.1568, took: 17.0621s\n",
            "  Batch 6199/7813, reward: -1.396, loss: 0.1638, took: 17.4981s\n",
            "  Batch 6299/7813, reward: -1.401, loss: 0.1986, took: 17.0024s\n",
            "  Batch 6399/7813, reward: -1.404, loss: 0.1435, took: 16.9967s\n",
            "  Batch 6499/7813, reward: -1.411, loss: 0.1168, took: 17.2677s\n",
            "  Batch 6599/7813, reward: -1.414, loss: 0.1738, took: 16.9773s\n",
            "  Batch 6699/7813, reward: -1.420, loss: 0.1777, took: 17.7677s\n",
            "  Batch 6799/7813, reward: -1.420, loss: 0.1441, took: 16.9314s\n",
            "  Batch 6899/7813, reward: -1.424, loss: 0.0915, took: 17.3509s\n",
            "  Batch 6999/7813, reward: -1.425, loss: 0.0831, took: 17.1163s\n",
            "  Batch 7099/7813, reward: -1.429, loss: 0.0982, took: 16.6722s\n",
            "  Batch 7199/7813, reward: -1.437, loss: 0.1034, took: 17.7546s\n",
            "  Batch 7299/7813, reward: -1.441, loss: 0.1144, took: 16.9864s\n",
            "  Batch 7399/7813, reward: -1.441, loss: 0.1272, took: 17.5902s\n",
            "  Batch 7499/7813, reward: -1.444, loss: 0.1206, took: 16.5971s\n",
            "  Batch 7599/7813, reward: -1.446, loss: 0.0934, took: 17.0573s\n",
            "  Batch 7699/7813, reward: -1.449, loss: 0.0683, took: 17.6742s\n",
            "  Batch 7799/7813, reward: -1.455, loss: 0.0834, took: 17.0254s\n",
            "Mean epoch loss/reward: 0.3071, -1.3115, -1.5520, 3.8501 took: 1376.4072s (17.3960s / 100 batches)\n",
            "\n",
            "  Batch 99/7813, reward: -1.455, loss: 0.0762, took: 17.1189s\n",
            "  Batch 199/7813, reward: -1.459, loss: 0.1032, took: 17.3295s\n",
            "  Batch 299/7813, reward: -1.460, loss: 0.0790, took: 16.7563s\n",
            "  Batch 399/7813, reward: -1.468, loss: 0.0767, took: 17.0818s\n",
            "  Batch 499/7813, reward: -1.470, loss: 0.0679, took: 17.7235s\n",
            "  Batch 599/7813, reward: -1.470, loss: 0.0561, took: 16.9057s\n",
            "  Batch 699/7813, reward: -1.478, loss: 0.0821, took: 17.6208s\n",
            "  Batch 799/7813, reward: -1.474, loss: 0.1108, took: 17.0949s\n",
            "  Batch 899/7813, reward: -1.476, loss: 0.0570, took: 17.8186s\n",
            "  Batch 999/7813, reward: -1.478, loss: 0.0827, took: 17.1549s\n",
            "  Batch 1099/7813, reward: -1.481, loss: 0.0593, took: 16.7580s\n",
            "  Batch 1199/7813, reward: -1.489, loss: 0.0860, took: 17.6099s\n",
            "  Batch 1299/7813, reward: -1.488, loss: 0.0660, took: 16.7724s\n",
            "  Batch 1399/7813, reward: -1.483, loss: 0.0559, took: 17.5208s\n",
            "  Batch 1499/7813, reward: -1.493, loss: 0.0702, took: 16.7204s\n",
            "  Batch 1599/7813, reward: -1.495, loss: 0.0855, took: 16.9280s\n",
            "  Batch 1699/7813, reward: -1.497, loss: 0.0759, took: 17.3979s\n",
            "  Batch 1799/7813, reward: -1.497, loss: 0.0512, took: 16.7989s\n",
            "  Batch 1899/7813, reward: -1.501, loss: 0.0668, took: 17.6788s\n",
            "  Batch 1999/7813, reward: -1.500, loss: 0.0567, took: 16.7293s\n",
            "  Batch 2099/7813, reward: -1.499, loss: 0.0529, took: 17.0718s\n",
            "  Batch 2199/7813, reward: -1.506, loss: 0.0488, took: 17.4745s\n",
            "  Batch 2299/7813, reward: -1.507, loss: 0.0297, took: 16.8889s\n",
            "  Batch 2399/7813, reward: -1.507, loss: 0.0683, took: 17.2106s\n",
            "  Batch 2499/7813, reward: -1.514, loss: 0.0374, took: 16.6428s\n",
            "  Batch 2599/7813, reward: -1.513, loss: 0.0388, took: 17.1498s\n",
            "  Batch 2699/7813, reward: -1.514, loss: 0.0339, took: 17.0801s\n",
            "  Batch 2799/7813, reward: -1.514, loss: 0.0327, took: 16.6238s\n",
            "  Batch 2899/7813, reward: -1.517, loss: 0.0338, took: 17.2592s\n",
            "  Batch 2999/7813, reward: -1.520, loss: 0.0534, took: 17.0509s\n",
            "  Batch 3099/7813, reward: -1.519, loss: 0.0360, took: 17.2709s\n",
            "  Batch 3199/7813, reward: -1.516, loss: 0.0558, took: 17.0818s\n",
            "  Batch 3299/7813, reward: -1.513, loss: 0.0365, took: 16.8078s\n",
            "  Batch 3399/7813, reward: -1.519, loss: 0.0324, took: 17.3851s\n",
            "  Batch 3499/7813, reward: -1.528, loss: 0.0334, took: 16.8648s\n",
            "  Batch 3599/7813, reward: -1.529, loss: 0.0433, took: 17.4183s\n",
            "  Batch 3699/7813, reward: -1.521, loss: 0.0246, took: 17.0840s\n",
            "  Batch 3799/7813, reward: -1.529, loss: 0.0252, took: 16.6242s\n",
            "  Batch 3899/7813, reward: -1.526, loss: 0.0376, took: 17.2918s\n",
            "  Batch 3999/7813, reward: -1.531, loss: 0.0190, took: 16.6979s\n",
            "  Batch 4099/7813, reward: -1.522, loss: 0.0318, took: 17.2192s\n",
            "  Batch 4199/7813, reward: -1.534, loss: 0.0412, took: 17.1445s\n",
            "  Batch 4299/7813, reward: -1.533, loss: 0.0454, took: 16.9785s\n",
            "  Batch 4399/7813, reward: -1.536, loss: 0.0465, took: 17.4213s\n",
            "  Batch 4499/7813, reward: -1.532, loss: 0.0270, took: 16.7195s\n",
            "  Batch 4599/7813, reward: -1.534, loss: 0.0526, took: 17.0331s\n",
            "  Batch 4699/7813, reward: -1.529, loss: 0.0025, took: 17.0423s\n",
            "  Batch 4799/7813, reward: -1.514, loss: 0.0337, took: 16.8020s\n",
            "  Batch 4899/7813, reward: -1.511, loss: 0.0240, took: 34.6707s\n",
            "  Batch 4999/7813, reward: -1.520, loss: 0.0251, took: 17.7339s\n",
            "  Batch 5099/7813, reward: -1.524, loss: 0.0377, took: 17.0934s\n",
            "  Batch 5199/7813, reward: -1.522, loss: 0.0428, took: 17.1483s\n",
            "  Batch 5299/7813, reward: -1.519, loss: 0.0222, took: 17.7810s\n",
            "  Batch 5399/7813, reward: -1.516, loss: 0.0260, took: 17.2681s\n",
            "  Batch 5499/7813, reward: -1.520, loss: 0.0203, took: 17.7286s\n",
            "  Batch 5599/7813, reward: -1.522, loss: 0.0214, took: 17.2294s\n",
            "  Batch 5699/7813, reward: -1.524, loss: 0.0124, took: 17.6090s\n",
            "  Batch 5799/7813, reward: -1.523, loss: 0.0309, took: 17.4293s\n",
            "  Batch 5899/7813, reward: -1.514, loss: 0.0326, took: 16.9909s\n",
            "  Batch 5999/7813, reward: -1.515, loss: 0.0259, took: 17.6305s\n",
            "  Batch 6099/7813, reward: -1.518, loss: 0.0232, took: 17.0993s\n",
            "  Batch 6199/7813, reward: -1.520, loss: 0.0544, took: 17.7798s\n",
            "  Batch 6299/7813, reward: -1.524, loss: 0.0421, took: 17.1701s\n",
            "  Batch 6399/7813, reward: -1.524, loss: 0.0190, took: 17.2435s\n",
            "  Batch 6499/7813, reward: -1.530, loss: 0.0254, took: 17.0298s\n",
            "  Batch 6599/7813, reward: -1.525, loss: 0.0265, took: 17.0374s\n",
            "  Batch 6699/7813, reward: -1.531, loss: 0.0077, took: 17.6489s\n",
            "  Batch 6799/7813, reward: -1.534, loss: 0.0165, took: 17.1787s\n",
            "  Batch 6899/7813, reward: -1.531, loss: 0.0175, took: 17.7390s\n",
            "  Batch 6999/7813, reward: -1.546, loss: 0.0170, took: 17.0449s\n",
            "  Batch 7099/7813, reward: -1.537, loss: -0.0079, took: 17.4034s\n",
            "  Batch 7199/7813, reward: -1.537, loss: 0.0423, took: 17.3052s\n",
            "  Batch 7299/7813, reward: -1.533, loss: -0.0038, took: 16.7785s\n",
            "  Batch 7399/7813, reward: -1.536, loss: 0.0323, took: 17.7174s\n",
            "  Batch 7499/7813, reward: -1.530, loss: 0.0334, took: 16.8194s\n",
            "  Batch 7599/7813, reward: -1.530, loss: 0.0059, took: 17.6596s\n",
            "  Batch 7699/7813, reward: -1.534, loss: 0.0098, took: 17.1467s\n",
            "  Batch 7799/7813, reward: -1.530, loss: 0.0116, took: 17.1252s\n",
            "Mean epoch loss/reward: 0.0411, -1.5124, -1.5637, 3.8501 took: 1377.2114s (17.4115s / 100 batches)\n",
            "\n",
            "  Batch 99/7813, reward: -1.526, loss: 0.0035, took: 17.0992s\n",
            "  Batch 199/7813, reward: -1.538, loss: 0.0090, took: 17.4468s\n",
            "  Batch 299/7813, reward: -1.539, loss: 0.0310, took: 17.0618s\n",
            "  Batch 399/7813, reward: -1.534, loss: 0.0031, took: 17.4139s\n",
            "  Batch 499/7813, reward: -1.532, loss: 0.0273, took: 17.1028s\n",
            "  Batch 599/7813, reward: -1.530, loss: 0.0076, took: 17.5495s\n",
            "  Batch 699/7813, reward: -1.529, loss: 0.0055, took: 17.0903s\n",
            "  Batch 799/7813, reward: -1.532, loss: -0.0049, took: 17.0141s\n",
            "  Batch 899/7813, reward: -1.536, loss: 0.0171, took: 17.8721s\n",
            "  Batch 999/7813, reward: -1.536, loss: 0.0173, took: 16.9377s\n",
            "  Batch 1099/7813, reward: -1.532, loss: 0.0101, took: 17.4975s\n",
            "  Batch 1199/7813, reward: -1.539, loss: -0.0026, took: 17.3805s\n",
            "  Batch 1299/7813, reward: -1.539, loss: 0.0309, took: 17.3663s\n",
            "  Batch 1399/7813, reward: -1.536, loss: 0.0055, took: 16.9156s\n",
            "  Batch 1499/7813, reward: -1.533, loss: 0.0040, took: 16.7482s\n",
            "  Batch 1599/7813, reward: -1.533, loss: 0.0142, took: 17.9236s\n",
            "  Batch 1699/7813, reward: -1.538, loss: 0.0023, took: 16.9369s\n",
            "  Batch 1799/7813, reward: -1.527, loss: 0.0215, took: 17.1423s\n",
            "  Batch 1899/7813, reward: -1.531, loss: 0.0111, took: 16.9555s\n",
            "  Batch 1999/7813, reward: -1.539, loss: 0.0147, took: 17.1412s\n",
            "  Batch 2099/7813, reward: -1.546, loss: 0.0201, took: 17.8050s\n",
            "  Batch 2199/7813, reward: -1.545, loss: 0.0101, took: 16.7860s\n",
            "  Batch 2299/7813, reward: -1.545, loss: 0.0031, took: 17.6177s\n",
            "  Batch 2399/7813, reward: -1.547, loss: 0.0103, took: 16.9128s\n",
            "  Batch 2499/7813, reward: -1.544, loss: -0.0038, took: 17.1759s\n",
            "  Batch 2599/7813, reward: -1.543, loss: 0.0059, took: 17.5778s\n",
            "  Batch 2699/7813, reward: -1.539, loss: 0.0082, took: 16.8289s\n",
            "  Batch 2799/7813, reward: -1.546, loss: 0.0131, took: 17.5247s\n",
            "  Batch 2899/7813, reward: -1.547, loss: 0.0131, took: 16.8210s\n",
            "  Batch 2999/7813, reward: -1.545, loss: 0.0293, took: 17.2949s\n",
            "  Batch 3099/7813, reward: -1.546, loss: 0.0180, took: 17.1017s\n",
            "  Batch 3199/7813, reward: -1.541, loss: 0.0061, took: 16.7046s\n",
            "  Batch 3299/7813, reward: -1.537, loss: 0.0128, took: 17.5529s\n",
            "  Batch 3399/7813, reward: -1.539, loss: 0.0078, took: 17.1366s\n",
            "  Batch 3499/7813, reward: -1.540, loss: 0.0172, took: 17.3130s\n",
            "  Batch 3599/7813, reward: -1.547, loss: 0.0114, took: 17.1023s\n",
            "  Batch 3699/7813, reward: -1.543, loss: 0.0050, took: 17.1585s\n",
            "  Batch 3799/7813, reward: -1.549, loss: 0.0198, took: 17.7197s\n",
            "  Batch 3899/7813, reward: -1.553, loss: 0.0057, took: 16.8993s\n",
            "  Batch 3999/7813, reward: -1.546, loss: 0.0155, took: 17.6433s\n",
            "  Batch 4099/7813, reward: -1.548, loss: -0.0014, took: 16.7586s\n",
            "  Batch 4199/7813, reward: -1.556, loss: 0.0202, took: 17.4469s\n",
            "  Batch 4299/7813, reward: -1.558, loss: 0.0046, took: 17.3359s\n",
            "  Batch 4399/7813, reward: -1.566, loss: 0.0110, took: 16.6525s\n",
            "  Batch 4499/7813, reward: -1.570, loss: 0.0111, took: 17.6243s\n",
            "  Batch 4599/7813, reward: -1.563, loss: 0.0197, took: 16.6017s\n",
            "  Batch 4699/7813, reward: -1.570, loss: 0.0113, took: 17.4943s\n",
            "  Batch 4799/7813, reward: -1.568, loss: 0.0077, took: 17.0119s\n",
            "  Batch 4899/7813, reward: -1.573, loss: 0.0185, took: 16.5959s\n",
            "  Batch 4999/7813, reward: -1.567, loss: 0.0092, took: 17.7519s\n",
            "  Batch 5099/7813, reward: -1.571, loss: 0.0223, took: 16.9088s\n",
            "  Batch 5199/7813, reward: -1.563, loss: 0.0085, took: 17.4424s\n",
            "  Batch 5299/7813, reward: -1.557, loss: -0.0048, took: 16.7435s\n",
            "  Batch 5399/7813, reward: -1.559, loss: 0.0197, took: 17.0926s\n",
            "  Batch 5499/7813, reward: -1.550, loss: -0.0026, took: 17.5294s\n",
            "  Batch 5599/7813, reward: -1.551, loss: 0.0080, took: 17.0443s\n",
            "  Batch 5699/7813, reward: -1.552, loss: 0.0183, took: 17.6217s\n",
            "  Batch 5799/7813, reward: -1.546, loss: 0.0188, took: 16.8723s\n",
            "  Batch 5899/7813, reward: -1.551, loss: 0.0226, took: 17.5105s\n",
            "  Batch 5999/7813, reward: -1.552, loss: 0.0098, took: 16.9356s\n",
            "  Batch 6099/7813, reward: -1.550, loss: 0.0349, took: 16.8792s\n",
            "  Batch 6199/7813, reward: -1.552, loss: 0.0282, took: 17.5763s\n",
            "  Batch 6299/7813, reward: -1.556, loss: 0.0190, took: 34.2929s\n",
            "  Batch 6399/7813, reward: -1.556, loss: 0.0154, took: 16.9780s\n",
            "  Batch 6499/7813, reward: -1.557, loss: 0.0323, took: 16.6209s\n",
            "  Batch 6599/7813, reward: -1.558, loss: 0.0140, took: 17.7419s\n",
            "  Batch 6699/7813, reward: -1.549, loss: 0.0260, took: 17.1522s\n",
            "  Batch 6799/7813, reward: -1.545, loss: 0.0165, took: 17.6240s\n",
            "  Batch 6899/7813, reward: -1.549, loss: 0.0275, took: 16.8588s\n",
            "  Batch 6999/7813, reward: -1.554, loss: 0.0112, took: 16.9227s\n",
            "  Batch 7099/7813, reward: -1.551, loss: 0.0113, took: 17.5120s\n",
            "  Batch 7199/7813, reward: -1.555, loss: 0.0144, took: 17.1220s\n",
            "  Batch 7299/7813, reward: -1.555, loss: 0.0135, took: 17.7901s\n",
            "  Batch 7399/7813, reward: -1.558, loss: 0.0048, took: 17.0863s\n",
            "  Batch 7499/7813, reward: -1.562, loss: 0.0106, took: 17.5336s\n",
            "  Batch 7599/7813, reward: -1.565, loss: 0.0176, took: 16.8185s\n",
            "  Batch 7699/7813, reward: -1.570, loss: 0.0216, took: 17.0672s\n",
            "  Batch 7799/7813, reward: -1.560, loss: 0.0151, took: 17.5572s\n",
            "Mean epoch loss/reward: 0.0132, -1.5482, -1.5859, 3.8501 took: 1378.3745s (17.4280s / 100 batches)\n",
            "\n",
            "  Batch 99/7813, reward: -1.570, loss: 0.0169, took: 17.8643s\n",
            "  Batch 199/7813, reward: -1.573, loss: 0.0069, took: 16.8129s\n",
            "  Batch 299/7813, reward: -1.572, loss: 0.0154, took: 17.4598s\n",
            "  Batch 399/7813, reward: -1.576, loss: 0.0163, took: 33.5295s\n",
            "  Batch 499/7813, reward: -1.575, loss: 0.0113, took: 17.3646s\n",
            "  Batch 599/7813, reward: -1.572, loss: 0.0299, took: 17.1636s\n",
            "  Batch 699/7813, reward: -1.577, loss: 0.0116, took: 17.6068s\n",
            "  Batch 799/7813, reward: -1.565, loss: 0.0207, took: 17.0157s\n",
            "  Batch 899/7813, reward: -1.574, loss: 0.0015, took: 17.2303s\n",
            "  Batch 999/7813, reward: -1.573, loss: 0.0106, took: 16.9672s\n",
            "  Batch 1099/7813, reward: -1.577, loss: 0.0209, took: 17.0241s\n",
            "  Batch 1199/7813, reward: -1.569, loss: 0.0131, took: 17.2372s\n",
            "  Batch 1299/7813, reward: -1.574, loss: 0.0219, took: 17.0001s\n",
            "  Batch 1399/7813, reward: -1.578, loss: 0.0114, took: 17.5819s\n",
            "  Batch 1499/7813, reward: -1.580, loss: 0.0171, took: 17.1531s\n",
            "  Batch 1599/7813, reward: -1.581, loss: 0.0110, took: 16.9917s\n",
            "  Batch 1699/7813, reward: -1.581, loss: 0.0044, took: 17.6119s\n",
            "  Batch 1799/7813, reward: -1.580, loss: 0.0165, took: 16.8444s\n",
            "  Batch 1899/7813, reward: -1.584, loss: 0.0238, took: 17.4653s\n",
            "  Batch 1999/7813, reward: -1.577, loss: 0.0079, took: 16.8540s\n",
            "  Batch 2099/7813, reward: -1.582, loss: 0.0176, took: 16.9859s\n",
            "  Batch 2199/7813, reward: -1.580, loss: 0.0123, took: 17.1640s\n",
            "  Batch 2299/7813, reward: -1.584, loss: 0.0272, took: 16.9048s\n",
            "  Batch 2399/7813, reward: -1.582, loss: 0.0077, took: 17.8809s\n",
            "  Batch 2499/7813, reward: -1.579, loss: 0.0085, took: 17.3124s\n",
            "  Batch 2599/7813, reward: -1.577, loss: 0.0078, took: 34.5733s\n",
            "  Batch 2699/7813, reward: -1.584, loss: 0.0181, took: 16.7362s\n",
            "  Batch 2799/7813, reward: -1.586, loss: 0.0132, took: 17.6699s\n",
            "  Batch 2899/7813, reward: -1.584, loss: 0.0001, took: 17.0062s\n",
            "  Batch 2999/7813, reward: -1.584, loss: 0.0134, took: 17.3218s\n",
            "  Batch 3099/7813, reward: -1.580, loss: 0.0119, took: 17.1050s\n",
            "  Batch 3199/7813, reward: -1.579, loss: 0.0153, took: 16.7931s\n",
            "  Batch 3299/7813, reward: -1.586, loss: 0.0143, took: 17.4599s\n",
            "  Batch 3399/7813, reward: -1.583, loss: 0.0056, took: 17.0239s\n",
            "  Batch 3499/7813, reward: -1.562, loss: 0.0016, took: 17.6272s\n",
            "  Batch 3599/7813, reward: -1.569, loss: -0.0068, took: 16.8566s\n",
            "  Batch 3699/7813, reward: -1.574, loss: 0.0079, took: 17.4636s\n",
            "  Batch 3799/7813, reward: -1.573, loss: 0.0125, took: 17.4932s\n",
            "  Batch 3899/7813, reward: -1.576, loss: 0.0069, took: 16.8389s\n",
            "  Batch 3999/7813, reward: -1.571, loss: 0.0045, took: 17.8186s\n",
            "  Batch 4099/7813, reward: -1.572, loss: 0.0076, took: 17.0560s\n",
            "  Batch 4199/7813, reward: -1.576, loss: 0.0096, took: 17.6869s\n",
            "  Batch 4299/7813, reward: -1.572, loss: 0.0167, took: 17.2336s\n",
            "  Batch 4399/7813, reward: -1.572, loss: 0.0024, took: 17.7781s\n",
            "  Batch 4499/7813, reward: -1.577, loss: 0.0165, took: 17.1798s\n",
            "  Batch 4599/7813, reward: -1.577, loss: 0.0028, took: 16.7400s\n",
            "  Batch 4699/7813, reward: -1.577, loss: -0.0027, took: 17.6353s\n",
            "  Batch 4799/7813, reward: -1.574, loss: 0.0133, took: 16.9103s\n",
            "  Batch 4899/7813, reward: -1.578, loss: 0.0117, took: 17.5388s\n",
            "  Batch 4999/7813, reward: -1.572, loss: 0.0107, took: 17.1332s\n",
            "  Batch 5099/7813, reward: -1.575, loss: 0.0049, took: 17.2344s\n",
            "  Batch 5199/7813, reward: -1.568, loss: 0.0033, took: 17.5688s\n",
            "  Batch 5299/7813, reward: -1.571, loss: 0.0080, took: 16.9511s\n",
            "  Batch 5399/7813, reward: -1.574, loss: 0.0011, took: 17.9963s\n",
            "  Batch 5499/7813, reward: -1.568, loss: 0.0108, took: 16.9676s\n",
            "  Batch 5599/7813, reward: -1.567, loss: -0.0004, took: 17.8353s\n",
            "  Batch 5699/7813, reward: -1.569, loss: 0.0133, took: 17.1441s\n",
            "  Batch 5799/7813, reward: -1.565, loss: 0.0013, took: 17.3085s\n",
            "  Batch 5899/7813, reward: -1.569, loss: 0.0152, took: 17.3253s\n",
            "  Batch 5999/7813, reward: -1.570, loss: 0.0083, took: 16.9097s\n",
            "  Batch 6099/7813, reward: -1.574, loss: 0.0018, took: 17.6338s\n",
            "  Batch 6199/7813, reward: -1.568, loss: 0.0057, took: 17.0792s\n",
            "  Batch 6299/7813, reward: -1.575, loss: 0.0174, took: 17.7508s\n",
            "  Batch 6399/7813, reward: -1.571, loss: -0.0047, took: 17.3821s\n",
            "  Batch 6499/7813, reward: -1.575, loss: -0.0028, took: 17.4476s\n",
            "  Batch 6599/7813, reward: -1.575, loss: 0.0118, took: 17.8777s\n",
            "  Batch 6699/7813, reward: -1.582, loss: 0.0121, took: 16.9695s\n",
            "  Batch 6799/7813, reward: -1.584, loss: 0.0169, took: 18.1016s\n",
            "  Batch 6899/7813, reward: -1.584, loss: 0.0068, took: 17.2106s\n",
            "  Batch 6999/7813, reward: -1.586, loss: 0.0104, took: 17.5813s\n",
            "  Batch 7099/7813, reward: -1.594, loss: 0.0072, took: 16.7813s\n",
            "  Batch 7199/7813, reward: -1.583, loss: 0.0085, took: 17.0223s\n",
            "  Batch 7299/7813, reward: -1.586, loss: 0.0082, took: 17.2779s\n",
            "  Batch 7399/7813, reward: -1.587, loss: 0.0187, took: 17.0679s\n",
            "  Batch 7499/7813, reward: -1.586, loss: 0.0177, took: 17.4192s\n",
            "  Batch 7599/7813, reward: -1.581, loss: 0.0026, took: 16.7572s\n",
            "  Batch 7699/7813, reward: -1.583, loss: 0.0088, took: 17.1444s\n",
            "  Batch 7799/7813, reward: -1.583, loss: 0.0027, took: 34.2311s\n",
            "Mean epoch loss/reward: 0.0101, -1.5767, -1.6135, 3.8501 took: 1417.2776s (17.9190s / 100 batches)\n",
            "\n",
            "  Batch 99/7813, reward: -1.586, loss: 0.0066, took: 17.0849s\n",
            "  Batch 199/7813, reward: -1.581, loss: 0.0166, took: 17.6001s\n",
            "  Batch 299/7813, reward: -1.587, loss: 0.0078, took: 16.7625s\n",
            "  Batch 399/7813, reward: -1.585, loss: 0.0067, took: 17.4917s\n",
            "  Batch 499/7813, reward: -1.588, loss: 0.0116, took: 16.7907s\n",
            "  Batch 599/7813, reward: -1.586, loss: 0.0082, took: 16.7006s\n",
            "  Batch 699/7813, reward: -1.587, loss: 0.0095, took: 17.2242s\n",
            "  Batch 799/7813, reward: -1.586, loss: 0.0099, took: 16.8301s\n",
            "  Batch 899/7813, reward: -1.587, loss: 0.0019, took: 17.6594s\n",
            "  Batch 999/7813, reward: -1.581, loss: 0.0002, took: 16.9049s\n",
            "  Batch 1099/7813, reward: -1.584, loss: 0.0167, took: 16.7916s\n",
            "  Batch 1199/7813, reward: -1.575, loss: 0.0095, took: 17.4404s\n",
            "  Batch 1299/7813, reward: -1.587, loss: 0.0141, took: 16.9888s\n",
            "  Batch 1399/7813, reward: -1.585, loss: 0.0109, took: 17.7703s\n",
            "  Batch 1499/7813, reward: -1.577, loss: 0.0134, took: 16.8471s\n",
            "  Batch 1599/7813, reward: -1.572, loss: 0.0079, took: 17.0416s\n",
            "  Batch 1699/7813, reward: -1.574, loss: 0.0037, took: 17.4235s\n",
            "  Batch 1799/7813, reward: -1.573, loss: 0.0100, took: 16.6426s\n",
            "  Batch 1899/7813, reward: -1.571, loss: 0.0056, took: 17.5823s\n",
            "  Batch 1999/7813, reward: -1.575, loss: 0.0032, took: 16.5682s\n",
            "  Batch 2099/7813, reward: -1.583, loss: 0.0080, took: 17.1323s\n",
            "  Batch 2199/7813, reward: -1.578, loss: 0.0199, took: 16.9872s\n",
            "  Batch 2299/7813, reward: -1.581, loss: 0.0028, took: 16.7108s\n",
            "  Batch 2399/7813, reward: -1.583, loss: 0.0134, took: 17.6647s\n",
            "  Batch 2499/7813, reward: -1.581, loss: 0.0141, took: 16.7946s\n",
            "  Batch 2599/7813, reward: -1.574, loss: 0.0201, took: 17.1320s\n",
            "  Batch 2699/7813, reward: -1.578, loss: 0.0069, took: 17.1137s\n",
            "  Batch 2799/7813, reward: -1.574, loss: 0.0076, took: 16.8241s\n",
            "  Batch 2899/7813, reward: -1.585, loss: 0.0171, took: 17.5889s\n",
            "  Batch 2999/7813, reward: -1.579, loss: 0.0050, took: 16.7578s\n",
            "  Batch 3099/7813, reward: -1.579, loss: 0.0162, took: 17.3645s\n",
            "  Batch 3199/7813, reward: -1.583, loss: 0.0096, took: 16.8312s\n",
            "  Batch 3299/7813, reward: -1.571, loss: 0.0109, took: 16.8460s\n",
            "  Batch 3399/7813, reward: -1.586, loss: 0.0173, took: 17.5921s\n",
            "  Batch 3499/7813, reward: -1.586, loss: 0.0068, took: 16.6750s\n",
            "  Batch 3599/7813, reward: -1.585, loss: 0.0152, took: 17.4354s\n",
            "  Batch 3699/7813, reward: -1.586, loss: 0.0083, took: 16.7450s\n",
            "  Batch 3799/7813, reward: -1.590, loss: 0.0157, took: 16.9541s\n",
            "  Batch 3899/7813, reward: -1.594, loss: 0.0177, took: 17.5214s\n",
            "  Batch 3999/7813, reward: -1.587, loss: 0.0184, took: 16.6888s\n",
            "  Batch 4099/7813, reward: -1.589, loss: 0.0115, took: 17.4701s\n",
            "  Batch 4199/7813, reward: -1.589, loss: 0.0068, took: 16.8166s\n",
            "  Batch 4299/7813, reward: -1.587, loss: 0.0094, took: 16.9125s\n",
            "  Batch 4399/7813, reward: -1.585, loss: 0.0095, took: 17.4667s\n",
            "  Batch 4499/7813, reward: -1.587, loss: 0.0021, took: 16.7843s\n",
            "  Batch 4599/7813, reward: -1.569, loss: 0.0074, took: 17.5971s\n",
            "  Batch 4699/7813, reward: -1.573, loss: 0.0083, took: 17.0653s\n",
            "  Batch 4799/7813, reward: -1.568, loss: -0.0116, took: 17.3585s\n",
            "  Batch 4899/7813, reward: -1.568, loss: -0.0081, took: 17.3984s\n",
            "  Batch 4999/7813, reward: -1.569, loss: 0.0030, took: 16.8029s\n",
            "  Batch 5099/7813, reward: -1.571, loss: 0.0020, took: 17.4779s\n",
            "  Batch 5199/7813, reward: -1.572, loss: 0.0036, took: 17.2042s\n",
            "  Batch 5299/7813, reward: -1.572, loss: 0.0118, took: 17.3969s\n",
            "  Batch 5399/7813, reward: -1.577, loss: 0.0122, took: 16.9940s\n",
            "  Batch 5499/7813, reward: -1.579, loss: 0.0054, took: 16.9683s\n",
            "  Batch 5599/7813, reward: -1.576, loss: 0.0055, took: 17.9246s\n",
            "  Batch 5699/7813, reward: -1.580, loss: 0.0107, took: 17.3473s\n",
            "  Batch 5799/7813, reward: -1.577, loss: 0.0065, took: 17.7017s\n",
            "  Batch 5899/7813, reward: -1.576, loss: 0.0008, took: 16.6997s\n",
            "  Batch 5999/7813, reward: -1.577, loss: 0.0127, took: 17.5889s\n",
            "  Batch 6099/7813, reward: -1.582, loss: 0.0118, took: 17.0854s\n",
            "  Batch 6199/7813, reward: -1.584, loss: 0.0071, took: 16.9056s\n",
            "  Batch 6299/7813, reward: -1.584, loss: 0.0069, took: 17.6464s\n",
            "  Batch 6399/7813, reward: -1.582, loss: 0.0159, took: 16.7994s\n",
            "  Batch 6499/7813, reward: -1.581, loss: 0.0088, took: 17.5996s\n",
            "  Batch 6599/7813, reward: -1.585, loss: 0.0105, took: 17.0817s\n",
            "  Batch 6699/7813, reward: -1.585, loss: 0.0032, took: 16.8791s\n",
            "  Batch 6799/7813, reward: -1.584, loss: 0.0112, took: 17.6454s\n",
            "  Batch 6899/7813, reward: -1.584, loss: 0.0071, took: 16.9194s\n",
            "  Batch 6999/7813, reward: -1.586, loss: 0.0047, took: 17.6041s\n",
            "  Batch 7099/7813, reward: -1.589, loss: 0.0006, took: 17.0691s\n",
            "  Batch 7199/7813, reward: -1.590, loss: 0.0103, took: 16.9014s\n",
            "  Batch 7299/7813, reward: -1.589, loss: 0.0104, took: 17.5360s\n",
            "  Batch 7399/7813, reward: -1.589, loss: 0.0002, took: 17.0618s\n",
            "  Batch 7499/7813, reward: -1.586, loss: 0.0084, took: 17.6126s\n",
            "  Batch 7599/7813, reward: -1.591, loss: 0.0141, took: 17.0207s\n",
            "  Batch 7699/7813, reward: -1.592, loss: 0.0126, took: 17.6314s\n",
            "  Batch 7799/7813, reward: -1.589, loss: 0.0123, took: 16.9421s\n",
            "Mean epoch loss/reward: 0.0088, -1.5816, -1.6110, 3.8501 took: 1357.9854s (17.1592s / 100 batches)\n",
            "\n",
            "Average tour length:  -1.579886555671692\n",
            "Average baseline tour length:  4.1225961446762085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C_u8Zy3Svuav"
      }
    }
  ]
}