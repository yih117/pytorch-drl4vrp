{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f2iqcoJQLAi",
        "outputId": "52965d9e-a656-44fb-8fad-2a5cf5cc40c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-drl4vrp'...\n",
            "warning: redirecting to https://github.com/yih117/pytorch-drl4vrp.git/\n",
            "remote: Enumerating objects: 173, done.\u001b[K\n",
            "remote: Counting objects: 100% (173/173), done.\u001b[K\n",
            "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
            "remote: Total 173 (delta 71), reused 169 (delta 70), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (173/173), 1.32 MiB | 26.57 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone http://github.com/yih117/pytorch-drl4vrp.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python pytorch-drl4vrp/trainer.py"
      ],
      "metadata": {
        "id": "AWqvm4e2Rl5J",
        "outputId": "4bb351fc-21ec-4cea-8919-840a8ce6183d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Batch 99/3907, reward: 9.563, loss: -14.4296, took: 13.6622s\n",
            "  Batch 199/3907, reward: 7.771, loss: -1.2434, took: 8.9582s\n",
            "  Batch 299/3907, reward: 7.610, loss: -1.3518, took: 9.0224s\n",
            "  Batch 399/3907, reward: 7.559, loss: -0.7322, took: 9.1029s\n",
            "  Batch 499/3907, reward: 7.517, loss: -0.1333, took: 8.9462s\n",
            "  Batch 599/3907, reward: 7.494, loss: 0.2098, took: 9.1768s\n",
            "  Batch 699/3907, reward: 7.486, loss: -0.8554, took: 9.3318s\n",
            "  Batch 799/3907, reward: 7.424, loss: 0.0365, took: 9.2531s\n",
            "  Batch 899/3907, reward: 7.222, loss: -0.0370, took: 9.0461s\n",
            "  Batch 999/3907, reward: 6.042, loss: -0.0699, took: 9.1743s\n",
            "  Batch 1099/3907, reward: 5.501, loss: -0.5912, took: 9.1848s\n",
            "  Batch 1199/3907, reward: 5.238, loss: -0.4132, took: 9.1866s\n",
            "  Batch 1299/3907, reward: 5.081, loss: -0.3254, took: 8.9552s\n",
            "  Batch 1399/3907, reward: 4.997, loss: -0.3977, took: 9.1430s\n",
            "  Batch 1499/3907, reward: 4.923, loss: -0.3214, took: 9.1535s\n",
            "  Batch 1599/3907, reward: 4.847, loss: -0.6556, took: 9.1852s\n",
            "  Batch 1699/3907, reward: 4.761, loss: -0.2026, took: 9.0576s\n",
            "  Batch 1799/3907, reward: 4.697, loss: -0.4050, took: 9.2048s\n",
            "  Batch 1899/3907, reward: 4.614, loss: -0.4728, took: 9.1726s\n",
            "  Batch 1999/3907, reward: 4.552, loss: -0.0075, took: 9.1522s\n",
            "  Batch 2099/3907, reward: 4.504, loss: -0.0238, took: 8.9935s\n",
            "  Batch 2199/3907, reward: 4.462, loss: -0.0916, took: 9.2525s\n",
            "  Batch 2299/3907, reward: 4.421, loss: -0.3230, took: 9.1610s\n",
            "  Batch 2399/3907, reward: 4.385, loss: -0.3572, took: 9.2150s\n",
            "  Batch 2499/3907, reward: 4.366, loss: -0.2512, took: 8.9934s\n",
            "  Batch 2599/3907, reward: 4.337, loss: 0.0401, took: 9.1996s\n",
            "  Batch 2699/3907, reward: 4.328, loss: -0.4249, took: 9.2021s\n",
            "  Batch 2799/3907, reward: 4.313, loss: -0.1768, took: 9.2875s\n",
            "  Batch 2899/3907, reward: 4.305, loss: -0.3135, took: 9.1627s\n",
            "  Batch 2999/3907, reward: 4.290, loss: -0.3508, took: 9.2741s\n",
            "  Batch 3099/3907, reward: 4.279, loss: -0.1239, took: 9.2952s\n",
            "  Batch 3199/3907, reward: 4.271, loss: -0.2164, took: 9.2446s\n",
            "  Batch 3299/3907, reward: 4.260, loss: -0.3776, took: 9.2462s\n",
            "  Batch 3399/3907, reward: 4.259, loss: -0.2574, took: 9.1680s\n",
            "  Batch 3499/3907, reward: 4.245, loss: -0.0956, took: 9.2373s\n",
            "  Batch 3599/3907, reward: 4.246, loss: -0.1798, took: 9.2460s\n",
            "  Batch 3699/3907, reward: 4.234, loss: -0.1982, took: 9.1570s\n",
            "  Batch 3799/3907, reward: 4.228, loss: -0.2052, took: 9.1762s\n",
            "  Batch 3899/3907, reward: 4.223, loss: -0.0649, took: 9.2799s\n",
            "Mean epoch loss/reward: -0.6761, 5.3020, 4.1371, took: 370.3962s (9.2785s / 100 batches)\n",
            "\n",
            "  Batch 99/3907, reward: 4.219, loss: -0.2140, took: 9.4146s\n",
            "  Batch 199/3907, reward: 4.229, loss: -0.0199, took: 9.2515s\n",
            "  Batch 299/3907, reward: 4.218, loss: -0.2288, took: 9.3478s\n",
            "  Batch 399/3907, reward: 4.212, loss: -0.2900, took: 9.3896s\n",
            "  Batch 499/3907, reward: 4.201, loss: -0.0043, took: 9.2756s\n",
            "  Batch 599/3907, reward: 4.208, loss: -0.1291, took: 9.1317s\n",
            "  Batch 699/3907, reward: 4.194, loss: -0.2252, took: 9.3117s\n",
            "  Batch 799/3907, reward: 4.192, loss: -0.1911, took: 9.2160s\n",
            "  Batch 899/3907, reward: 4.190, loss: -0.0898, took: 9.1549s\n",
            "  Batch 999/3907, reward: 4.186, loss: -0.1574, took: 9.0314s\n",
            "  Batch 1099/3907, reward: 4.178, loss: -0.2583, took: 9.2064s\n",
            "  Batch 1199/3907, reward: 4.189, loss: -0.1015, took: 9.2352s\n",
            "  Batch 1299/3907, reward: 4.179, loss: -0.0754, took: 9.2920s\n",
            "  Batch 1399/3907, reward: 4.176, loss: -0.1680, took: 9.0155s\n",
            "  Batch 1499/3907, reward: 4.176, loss: -0.1300, took: 9.1829s\n",
            "  Batch 1599/3907, reward: 4.178, loss: -0.1240, took: 9.1853s\n",
            "  Batch 1699/3907, reward: 4.188, loss: -0.2181, took: 9.2253s\n",
            "  Batch 1799/3907, reward: 4.173, loss: -0.1623, took: 8.9981s\n",
            "  Batch 1899/3907, reward: 4.171, loss: -0.0239, took: 9.2976s\n",
            "  Batch 1999/3907, reward: 4.170, loss: -0.1258, took: 9.2050s\n",
            "  Batch 2099/3907, reward: 4.163, loss: -0.0950, took: 9.1516s\n",
            "  Batch 2199/3907, reward: 4.165, loss: -0.0460, took: 8.9907s\n",
            "  Batch 2299/3907, reward: 4.156, loss: -0.0172, took: 9.1813s\n",
            "  Batch 2399/3907, reward: 4.157, loss: -0.0855, took: 9.1572s\n",
            "  Batch 2499/3907, reward: 4.153, loss: -0.0778, took: 9.2814s\n",
            "  Batch 2599/3907, reward: 4.156, loss: -0.1542, took: 8.9964s\n",
            "  Batch 2699/3907, reward: 4.155, loss: -0.1450, took: 9.2196s\n",
            "  Batch 2799/3907, reward: 4.157, loss: -0.1044, took: 9.2050s\n",
            "  Batch 2899/3907, reward: 4.151, loss: -0.0594, took: 9.2431s\n",
            "  Batch 2999/3907, reward: 4.156, loss: -0.0245, took: 9.0385s\n",
            "  Batch 3099/3907, reward: 4.151, loss: -0.0361, took: 9.2787s\n",
            "  Batch 3199/3907, reward: 4.147, loss: -0.0387, took: 9.2124s\n",
            "  Batch 3299/3907, reward: 4.141, loss: -0.1113, took: 9.1710s\n",
            "  Batch 3399/3907, reward: 4.137, loss: -0.1561, took: 9.0553s\n",
            "  Batch 3499/3907, reward: 4.139, loss: -0.1274, took: 9.1803s\n",
            "  Batch 3599/3907, reward: 4.145, loss: -0.1329, took: 9.2407s\n",
            "  Batch 3699/3907, reward: 4.140, loss: -0.1059, took: 9.3673s\n",
            "  Batch 3799/3907, reward: 4.143, loss: -0.1043, took: 9.1536s\n",
            "  Batch 3899/3907, reward: 4.137, loss: -0.1033, took: 9.0672s\n",
            "Mean epoch loss/reward: -0.1189, 4.1711, 4.0819, took: 366.8606s (9.1938s / 100 batches)\n",
            "\n",
            "  Batch 99/3907, reward: 4.134, loss: -0.0863, took: 9.3804s\n",
            "  Batch 199/3907, reward: 4.137, loss: -0.1422, took: 9.0132s\n",
            "  Batch 299/3907, reward: 4.135, loss: -0.1156, took: 9.2505s\n",
            "  Batch 399/3907, reward: 4.129, loss: -0.1182, took: 9.2364s\n",
            "  Batch 499/3907, reward: 4.136, loss: -0.0758, took: 9.2557s\n",
            "  Batch 599/3907, reward: 4.132, loss: -0.0967, took: 9.0437s\n",
            "  Batch 699/3907, reward: 4.128, loss: -0.0607, took: 9.1957s\n",
            "  Batch 799/3907, reward: 4.134, loss: -0.1213, took: 9.1352s\n",
            "  Batch 899/3907, reward: 4.124, loss: -0.1360, took: 9.1625s\n",
            "  Batch 999/3907, reward: 4.129, loss: -0.0678, took: 9.0317s\n",
            "  Batch 1099/3907, reward: 4.126, loss: -0.0871, took: 9.1319s\n",
            "  Batch 1199/3907, reward: 4.126, loss: -0.0848, took: 9.2329s\n",
            "  Batch 1299/3907, reward: 4.125, loss: -0.1307, took: 9.2395s\n",
            "  Batch 1399/3907, reward: 4.122, loss: -0.0882, took: 9.0977s\n",
            "  Batch 1499/3907, reward: 4.125, loss: -0.0789, took: 9.1238s\n",
            "  Batch 1599/3907, reward: 4.124, loss: -0.0935, took: 9.1768s\n",
            "  Batch 1699/3907, reward: 4.125, loss: -0.1111, took: 9.2969s\n",
            "  Batch 1799/3907, reward: 4.125, loss: -0.0961, took: 9.1454s\n",
            "  Batch 1899/3907, reward: 4.127, loss: -0.1285, took: 9.1334s\n",
            "  Batch 1999/3907, reward: 4.116, loss: -0.0682, took: 9.1557s\n",
            "  Batch 2099/3907, reward: 4.119, loss: -0.0851, took: 9.1598s\n",
            "  Batch 2199/3907, reward: 4.118, loss: -0.1239, took: 9.1417s\n",
            "  Batch 2299/3907, reward: 4.124, loss: -0.0858, took: 9.1092s\n",
            "  Batch 2399/3907, reward: 4.117, loss: -0.0576, took: 9.2390s\n",
            "  Batch 2499/3907, reward: 4.112, loss: -0.0913, took: 9.3217s\n",
            "  Batch 2599/3907, reward: 4.119, loss: -0.0911, took: 9.1505s\n",
            "  Batch 2699/3907, reward: 4.116, loss: -0.0913, took: 9.0057s\n",
            "  Batch 2799/3907, reward: 4.116, loss: -0.0911, took: 9.2494s\n",
            "  Batch 2899/3907, reward: 4.114, loss: -0.0872, took: 9.1315s\n",
            "  Batch 2999/3907, reward: 4.115, loss: -0.0845, took: 9.1518s\n",
            "  Batch 3099/3907, reward: 4.109, loss: -0.0764, took: 9.0591s\n",
            "  Batch 3199/3907, reward: 4.110, loss: -0.0991, took: 9.2177s\n",
            "  Batch 3299/3907, reward: 4.112, loss: -0.0371, took: 9.1592s\n",
            "  Batch 3399/3907, reward: 4.121, loss: -0.1294, took: 9.1371s\n",
            "  Batch 3499/3907, reward: 4.111, loss: -0.0926, took: 8.9562s\n",
            "  Batch 3599/3907, reward: 4.107, loss: -0.0772, took: 9.1713s\n",
            "  Batch 3699/3907, reward: 4.117, loss: -0.0691, took: 9.2216s\n",
            "  Batch 3799/3907, reward: 4.107, loss: -0.0659, took: 9.1485s\n",
            "  Batch 3899/3907, reward: 4.109, loss: -0.0572, took: 8.9331s\n",
            "Mean epoch loss/reward: -0.0917, 4.1213, 4.0596, took: 365.3488s (9.1565s / 100 batches)\n",
            "\n",
            "  Batch 99/3907, reward: 4.112, loss: -0.1201, took: 9.3948s\n",
            "  Batch 199/3907, reward: 4.105, loss: -0.0981, took: 9.0827s\n",
            "  Batch 299/3907, reward: 4.108, loss: -0.0553, took: 9.1235s\n",
            "  Batch 399/3907, reward: 4.114, loss: -0.0745, took: 9.3304s\n",
            "  Batch 499/3907, reward: 4.102, loss: -0.0783, took: 9.2570s\n",
            "  Batch 599/3907, reward: 4.105, loss: -0.0520, took: 9.1112s\n",
            "  Batch 699/3907, reward: 4.103, loss: -0.0958, took: 9.0557s\n",
            "  Batch 799/3907, reward: 4.108, loss: -0.0666, took: 9.1039s\n",
            "  Batch 899/3907, reward: 4.105, loss: -0.0781, took: 9.1116s\n",
            "  Batch 999/3907, reward: 4.103, loss: -0.0462, took: 9.1321s\n",
            "  Batch 1099/3907, reward: 4.101, loss: -0.0710, took: 8.9924s\n",
            "  Batch 1199/3907, reward: 4.103, loss: -0.0635, took: 9.1181s\n",
            "  Batch 1299/3907, reward: 4.098, loss: -0.0868, took: 9.1981s\n",
            "  Batch 1399/3907, reward: 4.104, loss: -0.0905, took: 9.1025s\n",
            "  Batch 1499/3907, reward: 4.102, loss: -0.0807, took: 9.0122s\n",
            "  Batch 1599/3907, reward: 4.106, loss: -0.0635, took: 9.2261s\n",
            "  Batch 1699/3907, reward: 4.104, loss: -0.0730, took: 9.1978s\n",
            "  Batch 1799/3907, reward: 4.097, loss: -0.0648, took: 9.1847s\n",
            "  Batch 1899/3907, reward: 4.105, loss: -0.0663, took: 9.1173s\n",
            "  Batch 1999/3907, reward: 4.102, loss: -0.0467, took: 9.2049s\n",
            "  Batch 2099/3907, reward: 4.102, loss: -0.0747, took: 9.1808s\n",
            "  Batch 2199/3907, reward: 4.100, loss: -0.0953, took: 9.1897s\n",
            "  Batch 2299/3907, reward: 4.101, loss: -0.0755, took: 9.0728s\n",
            "  Batch 2399/3907, reward: 4.099, loss: -0.0842, took: 9.1782s\n",
            "  Batch 2499/3907, reward: 4.101, loss: -0.0680, took: 9.2762s\n",
            "  Batch 2599/3907, reward: 4.094, loss: -0.0749, took: 9.1795s\n",
            "  Batch 2699/3907, reward: 4.092, loss: -0.0868, took: 9.0259s\n",
            "  Batch 2799/3907, reward: 4.098, loss: -0.0818, took: 9.2319s\n",
            "  Batch 2899/3907, reward: 4.100, loss: -0.0431, took: 9.1937s\n",
            "  Batch 2999/3907, reward: 4.098, loss: -0.0853, took: 9.1878s\n",
            "  Batch 3099/3907, reward: 4.098, loss: -0.0561, took: 9.0648s\n",
            "  Batch 3199/3907, reward: 4.094, loss: -0.0479, took: 9.1875s\n",
            "  Batch 3299/3907, reward: 4.094, loss: -0.1076, took: 9.1642s\n",
            "  Batch 3399/3907, reward: 4.090, loss: -0.0583, took: 9.1641s\n",
            "  Batch 3499/3907, reward: 4.097, loss: -0.0952, took: 9.0038s\n",
            "  Batch 3599/3907, reward: 4.093, loss: -0.0472, took: 9.1670s\n",
            "  Batch 3699/3907, reward: 4.104, loss: -0.0662, took: 9.2430s\n",
            "  Batch 3799/3907, reward: 4.092, loss: -0.0696, took: 9.1168s\n",
            "  Batch 3899/3907, reward: 4.095, loss: -0.0577, took: 8.9596s\n",
            "Mean epoch loss/reward: -0.0727, 4.1006, 4.0806, took: 364.9142s (9.1499s / 100 batches)\n",
            "\n",
            "  Batch 99/3907, reward: 4.097, loss: -0.0676, took: 9.3828s\n",
            "  Batch 199/3907, reward: 4.092, loss: -0.0589, took: 9.1124s\n",
            "  Batch 299/3907, reward: 4.086, loss: -0.0741, took: 9.0489s\n",
            "  Batch 399/3907, reward: 4.092, loss: -0.0727, took: 9.1887s\n",
            "  Batch 499/3907, reward: 4.093, loss: -0.0674, took: 9.1992s\n",
            "  Batch 599/3907, reward: 4.094, loss: -0.0695, took: 9.1276s\n",
            "  Batch 699/3907, reward: 4.088, loss: -0.0619, took: 9.0393s\n",
            "  Batch 799/3907, reward: 4.086, loss: -0.0968, took: 9.1309s\n",
            "  Batch 899/3907, reward: 4.091, loss: -0.0419, took: 9.1711s\n",
            "  Batch 999/3907, reward: 4.091, loss: -0.0739, took: 9.1826s\n",
            "  Batch 1099/3907, reward: 4.089, loss: -0.0702, took: 9.0141s\n",
            "  Batch 1199/3907, reward: 4.095, loss: -0.0770, took: 9.2120s\n",
            "  Batch 1299/3907, reward: 4.090, loss: -0.0739, took: 9.3143s\n",
            "  Batch 1399/3907, reward: 4.089, loss: -0.0761, took: 9.2600s\n",
            "  Batch 1499/3907, reward: 4.084, loss: -0.0764, took: 9.0384s\n",
            "  Batch 1599/3907, reward: 4.090, loss: -0.0432, took: 9.2139s\n",
            "  Batch 1699/3907, reward: 4.088, loss: -0.0616, took: 9.2104s\n",
            "  Batch 1799/3907, reward: 4.089, loss: -0.0485, took: 9.2083s\n",
            "  Batch 1899/3907, reward: 4.090, loss: -0.0479, took: 9.0548s\n",
            "  Batch 1999/3907, reward: 4.094, loss: -0.0782, took: 9.1468s\n",
            "  Batch 2099/3907, reward: 4.088, loss: -0.0668, took: 9.1615s\n",
            "  Batch 2199/3907, reward: 4.085, loss: -0.0672, took: 9.1822s\n",
            "  Batch 2299/3907, reward: 4.080, loss: -0.0634, took: 8.9603s\n",
            "  Batch 2399/3907, reward: 4.082, loss: -0.0504, took: 9.2280s\n",
            "  Batch 2499/3907, reward: 4.090, loss: -0.0677, took: 9.2322s\n",
            "  Batch 2599/3907, reward: 4.093, loss: -0.0615, took: 9.1347s\n",
            "  Batch 2699/3907, reward: 4.085, loss: -0.0196, took: 8.9661s\n",
            "  Batch 2799/3907, reward: 4.081, loss: -0.0700, took: 9.1360s\n",
            "  Batch 2899/3907, reward: 4.083, loss: -0.0384, took: 9.1726s\n",
            "  Batch 2999/3907, reward: 4.090, loss: -0.0552, took: 9.1276s\n",
            "  Batch 3099/3907, reward: 4.084, loss: -0.0521, took: 9.0410s\n",
            "  Batch 3199/3907, reward: 4.086, loss: -0.0528, took: 9.1776s\n",
            "  Batch 3299/3907, reward: 4.085, loss: -0.0465, took: 9.1741s\n",
            "  Batch 3399/3907, reward: 4.089, loss: -0.0744, took: 9.1872s\n",
            "  Batch 3499/3907, reward: 4.087, loss: -0.0659, took: 9.0149s\n",
            "  Batch 3599/3907, reward: 4.084, loss: -0.0484, took: 9.1783s\n",
            "  Batch 3699/3907, reward: 4.081, loss: -0.0606, took: 9.2316s\n",
            "  Batch 3799/3907, reward: 4.082, loss: -0.0489, took: 9.1415s\n",
            "  Batch 3899/3907, reward: 4.084, loss: -0.0735, took: 9.0093s\n",
            "Mean epoch loss/reward: -0.0620, 4.0878, 4.0596, took: 364.7314s (9.1465s / 100 batches)\n",
            "\n",
            "  Batch 99/3907, reward: 4.083, loss: -0.0499, took: 9.3911s\n",
            "  Batch 199/3907, reward: 4.082, loss: -0.0687, took: 9.1119s\n",
            "  Batch 299/3907, reward: 4.083, loss: -0.0744, took: 9.0175s\n",
            "  Batch 399/3907, reward: 4.087, loss: -0.0670, took: 9.1497s\n",
            "  Batch 499/3907, reward: 4.077, loss: -0.0632, took: 9.1467s\n",
            "  Batch 599/3907, reward: 4.075, loss: -0.0725, took: 9.0719s\n",
            "  Batch 699/3907, reward: 4.076, loss: -0.0681, took: 9.0158s\n",
            "  Batch 799/3907, reward: 4.074, loss: -0.0547, took: 9.1386s\n",
            "  Batch 899/3907, reward: 4.079, loss: -0.0345, took: 9.1209s\n"
          ]
        }
      ]
    }
  ]
}